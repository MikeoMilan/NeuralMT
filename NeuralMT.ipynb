{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGRC-Nz8PCtj"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
        "\n",
        "zh_tokenizer = transformers.AutoTokenizer.from_pretrained(\"hfl/chinese-xlnet-base\")\n",
        "en_tokenizer = transformers.AutoTokenizer.from_pretrained(\"xlnet-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zh_tokenizer.convert_tokens_to_ids(\"<pad>\"), en_tokenizer.convert_tokens_to_ids(\"<pad>\")"
      ],
      "metadata": {
        "id": "nVrJYuQWPLOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EntityDataset:\n",
        "    def __init__(self, src, trg):\n",
        "        self.src = src\n",
        "        self.trg = trg\n",
        "        self.MAX_LEN = 128\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        src = self.src[item]\n",
        "        trg = self.trg[item]\n",
        "\n",
        "        mask = [1] * len(src)\n",
        "\n",
        "        src_padding_len = self.MAX_LEN - len(src)\n",
        "        trg_padding_len = self.MAX_LEN - len(trg)\n",
        "\n",
        "        src = src + ([5] * src_padding_len) \n",
        "        trg = trg + ([5] * trg_padding_len)\n",
        "\n",
        "        return {\n",
        "            \"src\": torch.tensor(src, dtype=torch.long),\n",
        "            \"trg\": torch.tensor(trg, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "def get_data_loader(en_path, zh_path):\n",
        "    src_ids = []\n",
        "    trg_ids = []\n",
        "\n",
        "    with open(en_path) as src:\n",
        "        data = src.readlines()\n",
        "        for ids in tqdm(data):\n",
        "            ids_list = ids.split(\" \")[:-1]\n",
        "            ids_list = [int(ids_list[i]) for i in range(len(ids_list))]\n",
        "            src_ids.append(ids_list)\n",
        "\n",
        "    with open(zh_path) as src:\n",
        "        data = src.readlines()\n",
        "        for ids in tqdm(data):\n",
        "            ids_list = ids.split(\" \")[:-1]\n",
        "            ids_list = [int(ids_list[i]) for i in range(len(ids_list))]\n",
        "            trg_ids.append(ids_list)\n",
        "    return src_ids, trg_ids"
      ],
      "metadata": {
        "id": "0-LfB7_bPX9p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim_model, max_len):\n",
        "        super().__init__()\n",
        "        pos_encoding = torch.zeros(max_len, dim_model)\n",
        "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
        "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
        "        \n",
        "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
        "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
        "        \n",
        "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
        "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
        "        \n",
        "        # Saving buffer (same as parameter without gradients needed)\n",
        "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
        "        \n",
        "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
        "        return self.pos_encoding[:token_embedding.size(0), :]"
      ],
      "metadata": {
        "id": "JG61kEBQYYPt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_size,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_len,\n",
        "        device,\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
        "        self.src_position_embedding = PositionalEncoding(embedding_size, max_len)\n",
        "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
        "        self.trg_position_embedding = PositionalEncoding(embedding_size, max_len)\n",
        "\n",
        "        self.device = device\n",
        "        self.transformer = nn.Transformer(\n",
        "            embedding_size,\n",
        "            num_heads,\n",
        "            num_encoder_layers,\n",
        "            num_decoder_layers,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
        "\n",
        "        # (N, src_len)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_seq_length, N = src.shape\n",
        "        trg_seq_length, N = trg.shape\n",
        "\n",
        "        word_src = self.src_word_embedding(src)\n",
        "        word_trg = self.trg_word_embedding(trg)\n",
        "\n",
        "        embed_src = self.dropout((word_src + self.src_position_embedding(src)))\n",
        "        embed_trg = self.dropout((word_trg + self.trg_position_embedding(trg)))\n",
        "\n",
        "        src_padding_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(self.device)\n",
        "\n",
        "        out = self.transformer(\n",
        "            embed_src,\n",
        "            embed_trg,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_mask=trg_mask,\n",
        "        )\n",
        "        out = self.fc_out(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "Mimj95auPckh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_en, train_zh = get_data_loader(\"/content/drive/MyDrive/NLP/MT/train.en\", \"/content/drive/MyDrive/NLP/MT/train.zh\")\n",
        "valid_en, valid_zh = get_data_loader(\"/content/drive/MyDrive/NLP/MT/valid.en\", \"/content/drive/MyDrive/NLP/MT/valid.zh\")\n",
        "test_en, test_zh = get_data_loader(\"/content/drive/MyDrive/NLP/MT/test.en\", \"/content/drive/MyDrive/NLP/MT/test.zh\")"
      ],
      "metadata": {
        "id": "GGHYFMQcPfMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3492f3-8452-438a-cd35-97dd881ca8d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 279916/279916 [00:02<00:00, 104752.96it/s]\n",
            "100%|██████████| 279916/279916 [00:03<00:00, 87830.04it/s]\n",
            "100%|██████████| 15512/15512 [00:00<00:00, 59045.81it/s]\n",
            "100%|██████████| 15512/15512 [00:00<00:00, 51645.07it/s]\n",
            "100%|██████████| 15626/15626 [00:00<00:00, 54216.89it/s]\n",
            "100%|██████████| 15626/15626 [00:00<00:00, 63505.97it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Batch_size = 32\n",
        "\n",
        "train_dataset = EntityDataset(train_en, train_zh)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=Batch_size)\n",
        "\n",
        "valid_dataset = EntityDataset(valid_en, valid_zh)\n",
        "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=Batch_size)\n",
        "\n",
        "test_dataset = EntityDataset(test_en, test_zh)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=Batch_size)"
      ],
      "metadata": {
        "id": "pNWMz1CfQL0I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "batch_size = 32\n",
        "num_epochs = 20\n",
        "learning_rate = 5e-4\n",
        "\n",
        "src_vocab_size = len(en_tokenizer.vocab)\n",
        "trg_vocab_size = len(zh_tokenizer.vocab)\n",
        "embedding_size = 512\n",
        "num_heads = 4\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "dropout = 0.30\n",
        "max_len = 128\n",
        "forward_expansion = 1024\n",
        "src_pad_idx = en_tokenizer.convert_tokens_to_ids(\"<pad>\")\n",
        "\n",
        "\n",
        "model = Transformer(\n",
        "        embedding_size,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_len,\n",
        "        device,\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "vGOnmTDMQn6n"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-4\n",
        "\n",
        "num_train_steps = int(\n",
        "    len(train_dataset) / batch_size * num_epochs\n",
        ")\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "trg_pad_idx = zh_tokenizer.convert_tokens_to_ids(\"<pad>\")\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = trg_pad_idx, label_smoothing=0.1)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer, \n",
        "    num_warmup_steps=135, \n",
        "    num_training_steps=num_train_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi5DrBpXR6wJ",
        "outputId": "6ffd4591-63f0-4e9d-8f16-db1267438b52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch {}/{}:\".format(epoch+1, num_epochs))\n",
        "\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    final_loss = 0\n",
        "    for batch_idx, batch in enumerate(tqdm(train_data_loader)):\n",
        "        inp_data = batch[\"src\"].permute(1,0).to(device)\n",
        "        target = batch[\"trg\"].permute(1,0).to(device)\n",
        "        output = model(inp_data, target[:-1, :]) \n",
        "        output = output.reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()        \n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        final_loss += loss.item()\n",
        "    train_loss = final_loss / len(train_data_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "\n",
        "    model.eval()\n",
        "    valid_losses = []\n",
        "    final_loss = 0\n",
        "    for batch_idx, batch in enumerate(tqdm(valid_data_loader)):\n",
        "        inp_data = batch[\"src\"].permute(1,0).to(device)\n",
        "        target = batch[\"trg\"].permute(1,0).to(device)\n",
        "        output = model(inp_data, target[:-1, :]) \n",
        "        output = output.reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "        loss = criterion(output, target)\n",
        "        final_loss += loss.item()\n",
        "    valid_loss = final_loss / len(valid_data_loader)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    print(\"train loss: {},  valid_loss: {}\".format(train_loss, valid_loss))\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        torch.save(model.state_dict(), \"model.bin\")\n",
        "        best_loss = valid_loss"
      ],
      "metadata": {
        "id": "f4lmLtbwUvXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hfRz64ftkbUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate\n",
        "\n",
        "model.eval()\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1)\n",
        "\n",
        "sgd_mt = open(\"sgd_mt.txt\", \"w\")\n",
        "\n",
        "for k, batch in enumerate(tqdm(test_data_loader)):\n",
        "  #batch = next(iter(test_data_loader))\n",
        "\n",
        "  inp_data = batch[\"src\"].permute(1,0).to(device)\n",
        "  outputs = [zh_tokenizer.convert_tokens_to_ids(\"<cls>\")]\n",
        "\n",
        "  for i in range(128):\n",
        "      trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          output = model(inp_data, trg_tensor)\n",
        "\n",
        "      best_guess = output.argmax(2)[-1, :].item()\n",
        "      outputs.append(best_guess)\n",
        "\n",
        "      if best_guess == zh_tokenizer.convert_tokens_to_ids(\"<sep>\"):\n",
        "          break\n",
        "\n",
        "  translated_sentence = zh_tokenizer.decode(outputs[1:-1])\n",
        "  sgd_mt.write(translated_sentence)\n",
        "  sgd_mt.write(\"\\n\")"
      ],
      "metadata": {
        "id": "kEHTk4tHtGEn"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "src_vocab_size = len(en_tokenizer.vocab)\n",
        "trg_vocab_size = len(zh_tokenizer.vocab)\n",
        "embedding_size = 512\n",
        "num_heads = 4\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "dropout = 0.3\n",
        "max_len = 128\n",
        "forward_expansion = 1024\n",
        "src_pad_idx = en_tokenizer.convert_tokens_to_ids(\"<pad>\")\n",
        "\n",
        "\n",
        "model = Transformer(\n",
        "        embedding_size,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_len,\n",
        "        device,\n",
        ").to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/NLP/MT/model.bin'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I26p-HasvwN",
        "outputId": "b6f132e8-ca26-46cd-be65-2e7aaa8c6b3b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(infer_sentence, model):\n",
        "  infer_sentence = [3] + en_tokenizer.convert_tokens_to_ids(en_tokenizer.tokenize(infer_sentence)) + [4] \n",
        "  infer_dataset = EntityDataset([infer_sentence], [infer_sentence])\n",
        "  infer_data_loader = torch.utils.data.DataLoader(infer_dataset, batch_size=1)\n",
        "\n",
        "  model.eval()\n",
        "  batch = next(iter(infer_data_loader))\n",
        "  inp_data = batch[\"src\"].permute(1,0).to(device)\n",
        "  outputs = [zh_tokenizer.convert_tokens_to_ids(\"<cls>\")]\n",
        "\n",
        "  for i in range(128):\n",
        "      trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          output = model(inp_data, trg_tensor)\n",
        "\n",
        "      best_guess = output.argmax(2)[-1, :].item()\n",
        "      outputs.append(best_guess)\n",
        "\n",
        "      if best_guess == zh_tokenizer.convert_tokens_to_ids(\"<sep>\"):\n",
        "          break\n",
        "\n",
        "  translated_sentence = zh_tokenizer.decode(outputs[1:-1])\n",
        "  return translated_sentence"
      ],
      "metadata": {
        "id": "iQILnpVnCbuH"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zh_tokenizer.decode(o[1:-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n0vJa3ITHids",
        "outputId": "f977674f-ee96-45a0-f802-d262fa3a215d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'在福克斯新闻发布后,特朗普的推特开始。 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en1 = \"\"\"After a Fox News report, Mr. Trump’s tweets began.\"\"\"\n",
        "zh1 = \"在福克斯新闻报道后，特朗普先生的推文开始了。\"\n",
        "en2 = \"Paris - As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\"\n",
        "zh2 = \"巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。\"\n",
        "en3 = \"in 1989, liberal democracy triumphed over the socialist ideology incarnated and promoted by the Soviet Bloc.\"\n",
        "zh3 = \"1989年，自由民主战胜了由苏联集团具体化并推崇的社会主义意识形态。\"\n",
        "ZH1 = inference(en1, model)\n",
        "ZH2 = inference(en2, model)\n",
        "ZH3 = inference(en3, model)\n",
        "\n",
        "def show_mt(zh, ZH):\n",
        "  print(\"原始句子: {}\".format(zh))\n",
        "  print(\"机器翻译: {}\".format(ZH))\n",
        "  print(\"\\n\")\n",
        "\n",
        "show_mt(zh1, ZH1)\n",
        "show_mt(zh2, ZH2)\n",
        "show_mt(zh3, ZH3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6LuWW0aF1u2",
        "outputId": "1da24f75-7af4-47c5-f367-cd071715a847"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "原始句子: 在福克斯新闻报道后，特朗普先生的推文开始了。\n",
            "机器翻译: 在福克斯新闻发布后,特朗普的推特开始。 \n",
            "\n",
            "\n",
            "原始句子: 巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。\n",
            "机器翻译: 巴黎 - 随着经济危机深化和扩大,世界一直在寻找历史类图帮助我们知道发生了什么。 \n",
            "\n",
            "\n",
            "原始句子: 1989年，自由民主战胜了由苏联集团具体化并推崇的社会主义意识形态。\n",
            "机器翻译: 1989年,自由民主胜利在苏联解体所推动的社会主义意识形态上获胜。 \n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}